{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/0Nexus/0Nexus.github.io/blob/main/ngram_language_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Math, HTML\n",
        "display(HTML(\"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/\"\n",
        "               \"latest.js?config=default'></script>\"))"
      ],
      "metadata": {
        "id": "Q1kG1Wx9LJi5",
        "outputId": "ee3c50ce-cf43-4016-a14f-3ca27d9f802d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=default'></script>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Language models\n",
        "\n",
        "As we noted in the lecture today, language models are models that are trained to predict the likelihood of a sequence of words or characters in a language. The primary goal of language models is to learn the structure and patterns of a language by analyzing large amounts of text.\n",
        "\n",
        "\n",
        "We will begin our exploration by looking at the key fundamentals of the simplest language model.\n",
        "\n",
        "We will start of with a toy passage from Simple English Wikipedia."
      ],
      "metadata": {
        "id": "2yNW9m8zEztF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# our toy text (ideally we would train over large amounto of data, this is just a tiny snapshot)\n",
        "input_text = \"\"\"This is the front page of the Simple English Wikipedia.\n",
        "                Wikipedias are places where people work together to write encyclopedias\n",
        "                in different languages. We use Simple English words and grammar here.\n",
        "                The Simple English Wikipedia is for everyone! That includes children and\n",
        "                adults who are learning English. There are 227,530 articles on the Simple\n",
        "                English Wikipedia. All of the pages are free to use.\"\"\""
      ],
      "metadata": {
        "id": "HY-kSILPE7Bm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unigram language model\n",
        "\n",
        "Recall that the unigram language model is the simplest version of the language model, where we use the chain rule of probabilities and make a (very bad) independence assumption: we consider only the frequency (normalise this and obtain the probability) of individual words in a text and assume that each word is independent of other words (or contexts) in the sentence.\n",
        "\n",
        "To build a unigram model, the text is first split into individual words, and the frequency of each word is calculated. Then, the probability of each word is calculated by dividing its frequency by the total number of words in the text. This gives a probability distribution over the entire vocabulary of the language.\n",
        "\n",
        "Given a new text, a unigram model can predict the likelihood of each word in the vocabulary based on its frequency. To generate a sentence using a unigram model, words are sampled randomly from the unigram distribution. Given sufficient amount of data this can result in sentences that are grammatically correct but may not make sense semantically.\n",
        "\n",
        "In the code below: a sentence of length 10 is sampled from the unigram distribution using the choices() method from the random module."
      ],
      "metadata": {
        "id": "5UbxMGBNFvt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Split the text into a list of words\n",
        "words = input_text.split()\n",
        "\n",
        "# Count the frequency of each word and calculate the unigram probabilities\n",
        "unigram_counts = {}\n",
        "for word in words:\n",
        "    if word in unigram_counts:\n",
        "        unigram_counts[word] += 1\n",
        "    else:\n",
        "        unigram_counts[word] = 1\n",
        "\n",
        "num_words = len(words)\n",
        "unigram_probs = {word: count / num_words for word, count in unigram_counts.items()}\n",
        "\n",
        "# Sample a sentence of length 10 from the unigram distribution\n",
        "sampled_sentence = []\n",
        "for i in range(100):\n",
        "    sampled_word = random.choices(list(unigram_probs.keys()), list(unigram_probs.values()))[0]\n",
        "    sampled_sentence.append(sampled_word)\n",
        "\n",
        "# Print out the sampled sentence\n",
        "print(\"Sampled sentence:\")\n",
        "print(\" \".join(sampled_sentence))"
      ],
      "metadata": {
        "id": "Q9KQQrbIFp8X",
        "outputId": "de486946-b528-4b3b-bf5d-72ca41487a67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled sentence:\n",
            "This use children Simple Wikipedia All English Wikipedia. is Wikipedia. English. are English work and This Simple adults front people the All Wikipedias are where are the Wikipedia. who All of languages. English. everyone! page Wikipedia and children the pages of Simple is languages. write on are are free in the Simple to Simple here. Simple English Wikipedia to There English. 227,530 That includes All people Wikipedia here. words All are Simple There Simple use the use languages. of The Wikipedia includes Wikipedia. This Simple adults the English the Simple and grammar is are are words of Simple English is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO:\n",
        "\n",
        "1.   Try using a large sized input sample and see how the sampling changes.\n",
        "2.   Change the length of the generated samples\n",
        "3.   What happens to words that are not seen in the training set?\n",
        "4.   There is something interesting happening in the `else` loop above. What is this? Why is this done? What is the effect of not having this loop?\n",
        "\n"
      ],
      "metadata": {
        "id": "fGdzmOzWG0Dd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bigram language model\n",
        "\n",
        "Recall that the bigram language model predicts the probability of a word based on the probability of its preceding word. In other words, a bigram model considers the probability of a word based on the occurrence of its preceding word in the sentence.\n",
        "\n",
        "Notice in the code below the bigram counts are calculated by looping over each adjacent pair of words in the text. The frequency of each bigram is stored in a dictionary called bigram_counts."
      ],
      "metadata": {
        "id": "kQwlz0_fHb6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate bigram counts\n",
        "bigram_counts = {}\n",
        "for i in range(len(words)-1):\n",
        "    current_word = words[i]\n",
        "    next_word = words[i+1]\n",
        "    bigram = (current_word, next_word)\n",
        "    if bigram in bigram_counts:\n",
        "        bigram_counts[bigram] += 1\n",
        "    else:\n",
        "        bigram_counts[bigram] = 1\n",
        "\n",
        "# Calculate bigram probabilities\n",
        "num_bigrams = sum(bigram_counts.values())\n",
        "bigram_probs = {bigram: count / num_bigrams for bigram, count in bigram_counts.items()}\n",
        "\n",
        "# Sample a sentence of length 10 from the bigram distribution\n",
        "start_word = random.choice(words)\n",
        "sampled_sentence = [start_word]\n",
        "for i in range(100):\n",
        "    current_word = sampled_sentence[-1]\n",
        "    possible_bigrams = [bigram for bigram in bigram_probs.keys() if bigram[0] == current_word]\n",
        "    if len(possible_bigrams) > 0:\n",
        "        sampled_bigram = random.choices(possible_bigrams, [bigram_probs[bigram] for bigram in possible_bigrams])[0]\n",
        "        next_word = sampled_bigram[1]\n",
        "    else:\n",
        "        next_word = random.choice(words)\n",
        "    sampled_sentence.append(next_word)\n",
        "\n",
        "# Print out the sampled sentence\n",
        "print(\"Sampled sentence:\")\n",
        "print(\" \".join(sampled_sentence))\n"
      ],
      "metadata": {
        "id": "083NSk6wGxwU",
        "outputId": "44ee1618-14f5-4831-ab93-4a19cf75efab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled sentence:\n",
            "where people work together to write encyclopedias in different languages. We use Simple English Wikipedia. All of the Simple English Wikipedia. Wikipedias are free to use. Wikipedia. All of the front page of the Simple English Wikipedia. All of the Simple English Wikipedia. Wikipedias are free to write encyclopedias in different languages. We use Simple English Wikipedia. Wikipedias are places where people work together to write encyclopedias in different languages. We use Simple English words and adults who are 227,530 articles on the front page of the front page of the Simple English words and grammar here. The Simple English Wikipedia.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trigram language model\n",
        "\n",
        "Now let's do trigram language model where a trigram model considers the probability of a word based on the occurrence of its two preceding words in the sentence.\n",
        "\n",
        "It is very similar to the bigram language model and is an extension. It considers preceding two word context.\n"
      ],
      "metadata": {
        "id": "1zqsHruqIawM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate trigram counts\n",
        "trigram_counts = {}\n",
        "for i in range(len(words)-2):\n",
        "    current_word = words[i]\n",
        "    next_word = words[i+1]\n",
        "    third_word = words[i+2]\n",
        "    trigram = (current_word, next_word, third_word)\n",
        "    if trigram in trigram_counts:\n",
        "        trigram_counts[trigram] += 1\n",
        "    else:\n",
        "        trigram_counts[trigram] = 1\n",
        "\n",
        "# Calculate trigram probabilities\n",
        "num_trigrams = sum(trigram_counts.values())\n",
        "trigram_probs = {trigram: count / num_trigrams for trigram, count in trigram_counts.items()}\n",
        "\n",
        "# Sample a sentence of length 10 from the trigram distribution\n",
        "start_bigram = random.choices([(words[i], words[i+1]) for i in range(len(words)-1)], k=1)[0]\n",
        "sampled_sentence = [start_bigram[0], start_bigram[1]]\n",
        "for i in range(10):\n",
        "    current_bigram = (sampled_sentence[-2], sampled_sentence[-1])\n",
        "    possible_trigrams = [trigram for trigram in trigram_probs.keys() if trigram[:2] == current_bigram]\n",
        "    if len(possible_trigrams) > 0:\n",
        "        sampled_trigram = random.choices(possible_trigrams, [trigram_probs[trigram] for trigram in possible_trigrams])[0]\n",
        "        next_word = sampled_trigram[2]\n",
        "    else:\n",
        "        next_word = random.choice(words)\n",
        "    sampled_sentence.append(next_word)\n",
        "\n",
        "# Print out the sampled sentence\n",
        "print(\"Sampled sentence:\")\n",
        "print(\" \".join(sampled_sentence))"
      ],
      "metadata": {
        "id": "TZFuQSmfIX7b",
        "outputId": "1b53c521-269b-4fc5-9ca0-c4ca724e9f3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled sentence:\n",
            "are places where people work together to write encyclopedias in different languages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate trigram counts\n",
        "quadgram_counts = {}\n",
        "for i in range(len(words)-3):\n",
        "    current_word = words[i]\n",
        "    next_word = words[i+1]\n",
        "    third_word = words[i+2]\n",
        "    quad_word = words[i+3]\n",
        "    quadgram = (current_word, next_word, third_word, quad_word)\n",
        "    if quadgram in quadgram_counts:\n",
        "        quadgram_counts[quadgram] += 1\n",
        "    else:\n",
        "        quadgram_counts[quadgram] = 1\n",
        "\n",
        "# Calculate trigram probabilities\n",
        "num_quadgrams = sum(quadgram_counts.values())\n",
        "quadgram_probs = {quadgram: count / num_quadgrams for quadgram, count in quadgram_counts.items()}\n",
        "\n",
        "# Sample a sentence of length 10 from the trigram distribution\n",
        "start_trigram = random.choices([(words[i], words[i+1], words[i+2]) for i in range(len(words)-2)], k=2)[0]\n",
        "sampled_sentence = [start_trigram[0], start_trigram[1],start_trigram[2]]\n",
        "for i in range(10):\n",
        "    current_trigram = (sampled_sentence[-3], sampled_sentence[-2], sampled_sentence[-1])\n",
        "    possible_quadgram = [trigram for trigram in trigram_probs.keys() if trigram[:3] == current_bigram]\n",
        "    if len(possible_quadgram) > 0:\n",
        "        sampled_quadgram = random.choices(possible_quadgram, [quadgram_probs[quadgram] for quadgram in possible_quadgram])[0]\n",
        "        next_word = sampled_quadgram[3]\n",
        "    else:\n",
        "        next_word = random.choice(words)\n",
        "    sampled_sentence.append(next_word)\n",
        "\n",
        "# Print out the sampled sentence\n",
        "print(\"Sampled sentence:\")\n",
        "print(\" \".join(sampled_sentence))"
      ],
      "metadata": {
        "id": "_1eUZIRmFmN3",
        "outputId": "1774a666-930d-4a12-fe84-d4ea184ee23a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled sentence:\n",
            "Wikipedia. All of Wikipedia. pages free All pages pages the is of are\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO:\n",
        "\n",
        "1.   Try both the bigram and the trigram models with the larger dataset, do you see a difference in the generated samples? What is happening? Is the quality improving? Why is this happening?   \n",
        "2.   (Challenge excercise) Extend this for 4-gram language model. What does this involve?\n",
        "\n"
      ],
      "metadata": {
        "id": "NZGBz7wnJi3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating language models: Perplexity\n",
        "\n",
        "Recall that perplexity is a measure of how well a language model can predict a sequence of words. It is calculated as the inverse probability of the test set, normalized by the number of words in the test set, it is given as: $ppl(s) = 2^{-\\scriptscriptstyle{\\frac{\\log P(s)}{N}}}$,\n",
        "\n",
        " where $s$ is a sentence, $P(s)$ is the probability of the sentence according to the LM.\n",
        "\n",
        " We will define this below."
      ],
      "metadata": {
        "id": "njOypTdDKzvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def calculate_perplexity(test_set, ngram_probs):\n",
        "    # Split the test set into a list of words\n",
        "    test_words = test_set.split()\n",
        "\n",
        "    # Calculate the perplexity of the test set\n",
        "    N = len(test_words)\n",
        "    log_prob = 0\n",
        "    for i in range(len(test_words)-(len(ngram_probs)-1)):\n",
        "        ngram = tuple(test_words[i:i+len(ngram_probs)])\n",
        "        if ngram in ngram_probs:\n",
        "            log_prob += math.log(ngram_probs[ngram])\n",
        "        else:\n",
        "            # Add probability of unknown word\n",
        "            log_prob += math.log(1 / (sum(ngram_probs.values()) + 1))\n",
        "    perplexity = math.exp(-log_prob/N)\n",
        "\n",
        "    return perplexity"
      ],
      "metadata": {
        "id": "TlaLslg7JM9c"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code calculates the perplexity of a test set using a trigram language model. The test set is defined as the string \"this article is from Wikipedia\". Remember, lower the perplexity -- better the quality of the LM (when trained on language related data).\n",
        "\n",
        "Another way to think of perplexity is essentially asking to what extent is the LM surprised with a given sample of data.\n"
      ],
      "metadata": {
        "id": "p7o-OSTcMs_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the test set as a string\n",
        "test_set = \"this article is from Wikipedia\"\n",
        "\n",
        "# Calculate perplexity of test set using the trigram language model\n",
        "trigram_perplexity = calculate_perplexity(test_set, trigram_probs)\n",
        "print(\"Trigram perplexity:\", trigram_perplexity)\n",
        "# Define the test set as a string\n",
        "test_set = \"this article is from Wikipedia\"\n",
        "\n",
        "# Calculate perplexity of test set using the trigram language model\n",
        "quadgram_perplexity = calculate_perplexity(test_set, quadgram_probs)\n",
        "print(\"quadgram perplexity:\", quadgram_perplexity)\n",
        "# Define the test set as a string\n",
        "test_set = \"this article is from Wikipedia\"\n",
        "\n",
        "# Calculate perplexity of test set using the trigram language model\n",
        "bigram_perplexity = calculate_perplexity(test_set, bigram_probs)\n",
        "print(\"bigram perplexity:\", bigram_perplexity)\n",
        "\n",
        "# Calculate perplexity of test set using the trigram language model\n",
        "unigram_perplexity = calculate_perplexity(test_set, unigram_probs)\n",
        "print(\"Unigram perplexity:\", unigram_perplexity)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-F4O-tLyJXVp",
        "outputId": "e90cf5ac-70b6-4811-aa28-3e6869795b6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trigram perplexity: 1.0\n",
            "quadgram perplexity: 1.0\n",
            "bigram perplexity: 1.0\n",
            "Unigram perplexity: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO\n",
        "\n",
        "1. Can you compare the perplexity from each of the LMs that we have implemented?\n",
        "\n",
        "2. If you have constructed 4-gram language model, please evaluate the perplexity of the model.\n",
        "\n",
        "3. Can you go on in this way to {5, 6, 7, 9}-grams? Where is the bottleneck? What would extending the context window do?"
      ],
      "metadata": {
        "id": "eMjbB_T4M89w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HL_bJW87OWFf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}